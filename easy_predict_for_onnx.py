import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import pandas as pd
from onnxruntime import InferenceSession
from tqdm import tqdm
import json
import os
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import BertTokenizer, BertConfig, AutoConfig
import onnx
from onnxconverter_common import float16
from onnxmltools.utils.float16_converter import convert_float_to_float16_model_path
#!pip install transformers==4.9.2

class Config():
    slide_window=True
    add_cate=False
    model_path="/mnt/nj-larc/usr/ajie1/sunyifei/nlp_codebase/logs/train/SunyifeiTanghuang48LConfig/20250321_153747/checkpoints/0/step_55000"
    initial_pretrain_path="/mnt/public03/usr/sunyifei2/level_classification_newstd/checkpoints/chinese-bert-wwm-ext"
    num_classes=2

class oriPredictor:
    def __init__(self,conifg):
        self.config=config

        self.tokenizer=load_tokenizer()
        self.model_path=self.config.model_path
        self.initial_pretrain_path=self.config.initial_pretrain_path

        load_model()

        self.text_splitter=TextSplitter()

    
    def load_model(self):
        self.model=Bert
        self.model = self.model.from_pretrained(self.initial_pretrain_path)
        if not self.model.fc[-1].out_features == self.config.num_classes:
            print('The classifier layer reinitialized')
            self.model.fc[-1] = torch.nn.Linear(in_features=self.model.fc[-1].in_features, out_features=self.config.num_classes, bias=True)  
        path_model=os.path.join(self.model_path,"pytorch_model.bin")

        if os.path.isfile(path_model):
            msg=self.model.load_state_dict(torch.load(path_model))
            print(f">>>>>>>>>>>>>>>>>>load weight from {path_model}, with msg :{msg}") 
        else:
            raise Exception
    def _text_to_encoding(self, content, cate):
        # return self.tokenizer.tokenizer(content, max_lenth=400)
        return self.tokenizer.tokenizer(content, cate, max_length=self.max_length, padding='max_length', return_tensors="pt", truncation="only_first")
    def predict_one_data(self,data):
        note_info=data
        output_dict, fea_status = fea_process_fuc(note_info,self.tokenizer)  
        input_ids=torch.tensor(output_dict['ids'])
        attention_mask=torch.tensor(output_dict['att'])
        with torch.no_grad():
            output = self.model(input_ids, attention_mask)
        output=output[0]
        print(">>>>>>>>output")
        print(output)
        if self.config.slide_window:
            output=output.max(dim=0)[0]
            print(output)
        output = torch.nn.functional.softmax(output,dim=-1).cpu().detach().numpy()
        print(output)



        
class Predictor:
    def __init__(self,onnx_fp16_path):
        self.tokenizer=load_tokenizer()
        self.onnx_fp16_path=onnx_fp16_path
        self.load_sess()
    def load_sess(self):
        self.sess=InferenceSession(self.onnx_fp16_path,providers=['CUDAExecutionProvider'])
    def predict_one_data(self,data):
        note_info=data
        output_dict, fea_status = fea_process_fuc(note_info,self.tokenizer)
        #logger.info(str(output_dict))
        if output_dict is not None and output_dict != {}:
            input_data = {
                "INPUT_0":output_dict['ids'],
                "INPUT_1":output_dict['att']
            }
            taxonomy1 = output_dict['taxonomy1']
            taxonomy2 = output_dict['taxonomy2']
            prob_list = self.sess.run(output_names=["OUTPUT_0"],input_feed=input_data)
            prob_list=np.array(prob_list[0])
            # print(prob_list.shape)#(1, 1, 2)
            # prob_list=model_output['OUTPUT_0']
            score=self.gender_post_process(prob_list)
        else:
            pass       
        return score
    def predict_list(self,data_list,save_path):
        note_id_list=[]
        score_list=[]
        for data in tqdm(data_list):
            score=self.predict_data(data)
            note_id_list.append(data['note_id'])
            score_list.append(score)
        df=pd.DataFrame({'note_id':note_id_list,'score':score_list})
        df.to_csv(save_path,index=False)
    
    def gender_post_process(self, prob_list):
        prob_list = torch.tensor(prob_list)
        print(">>>>>>prob list")
        print(prob_list)
        if prob_list.shape[0] == 1:
            norm_prob_list = torch.nn.functional.softmax(prob_list,dim=1)
            norm_prob_list = norm_prob_list.tolist()
            #print(norm_prob_list)
            print('result:',norm_prob_list)
        else:
            prob_list = prob_list.max(dim=0)[0].unsqueeze(dim=0)
            norm_prob_list = torch.nn.functional.softmax(prob_list)
            norm_prob_list = norm_prob_list.tolist()
            print('result:',norm_prob_list)
        score = norm_prob_list[0][1]
        return score


def convert_fp16(onnx_model_path,onnx_model_fp16_path):
    new_onnx_model = convert_float_to_float16_model_path(onnx_model_path, keep_io_types=True)

    # Convert to FP16

    onnx.save(new_onnx_model, onnx_model_fp16_path, save_as_external_data=True)

    print(f"Converted ONNX model to FP16: {onnx_model_fp16_path}")

def test_fp_16(onnx_model_fp16_path,tokenizer,device="cuda"):
    content_list=[
        "inputs = tokenizer(content_list, return_tensors=, truncation=True, max_length=256, padding='max_length')inputs = tokenizer(content_list, return_tenso"", truncation=True, max_length=256, padding='max_length')inputs = tokenizer(content_list, return_tensors truncation=True, max_length=256, padding='max_length')"
    ]
    text_enc = tokenizer(content_list, return_tensors="pt", truncation=True, max_length=256, padding='max_length')
    encode_dict = {key: val.cpu().numpy() for key, val in text_enc.items()}

    input_ids = encode_dict['input_ids']
    input_masks = encode_dict['attention_mask']
    input_data = {
        "INPUT_0":input_ids,
        "INPUT_1":input_masks
    }    
    print(input_data)
    sess=InferenceSession(onnx_model_fp16_path,providers=['CUDAExecutionProvider'])
    prob_list = sess.run(output_names=["OUTPUT_0"],input_feed=input_data)
    prob_list=np.array(prob_list[0])
    print(prob_list)


def convert_onnx(model,tokenizer,onnx_path,device="cuda"):
    content_list=[
        "inputs = tokenizer(content_list, return_tensors=, truncation=True, max_length=256, padding='max_length')inputs = tokenizer(content_list, return_tenso"", truncation=True, max_length=256, padding='max_length')inputs = tokenizer(content_list, return_tensors truncation=True, max_length=256, padding='max_length')"
    ]
    inputs = tokenizer(content_list, return_tensors="pt", truncation=True, max_length=256, padding='max_length')
    input_ids = torch.LongTensor(inputs['input_ids']).to(device)
    attention_masks = torch.LongTensor(inputs['attention_mask']).to(device)
    model.to(device)
    dummpy_input = {
    "INPUT_0":  input_ids,
    "INPUT_1": attention_masks,
    }   
    torch.onnx.export(model, 
        tuple(dummpy_input.values()), 
        onnx_path, 
        export_params=True,
        opset_version=14,
        input_names=list(dummpy_input), 
        output_names = ['OUTPUT_0'],
        dynamic_axes= {'INPUT_0': {0: 'batch_size'},
                    'INPUT_1': {0: 'batch_size'},
                    'OUTPUT_0': {0: 'batch_size'}})
    
    print(f'>>>>>>>>>>>Model has been converted to ONNX {onnx_path}' )   


from module.ModelMap import map_model,map_tokenizer
from module.Trainer import load_state
def load_model():
    #model_name='Bert'large
    model_name='MegatronBert_t'
    initial_pretrain_model='/mnt/nj-larc/usr/ajie1/sunyifei/hugging_face/tanghuang_48L'  
    initial_pretrain_tokenizer='/mnt/nj-larc/usr/ajie1/sunyifei/hugging_face/tanghuang_48L'  
    path_model='/mnt/nj-larc/usr/ajie1/sunyifei/nlp_codebase/logs/train/SunyifeiTanghuang48LConfig/20250321_153747/checkpoints/0/step_55000/model.safetensors.index.json'
    model=None
    tokenizer=None
    if model_name in ('MegatronBert_t','XLMRoberta'):
        model = map_model(model_name)
        model_config = AutoConfig.from_pretrained(initial_pretrain_model, num_labels=5)  
        model=model.from_pretrained(initial_pretrain_model,config=model_config)
        # num_classes=2
        # if not model.fc[-1].out_features== num_classes:
        #     print(f"change num_classes!!!! {num_classes}")
        #     model.fc = torch.nn.Linear(in_features= model.classifier.in_features, out_features=num_classes, bias=True) 
        msg=model.load_state_dict(load_state(path_model))
        print("load msg",msg)


        tokenizer = map_tokenizer(model_name)
        tokenizer = tokenizer.from_pretrained(initial_pretrain_tokenizer)
    else:
        tokenizer=BertTokenizer.from_pretrained(initial_pretrain_tokenizer)
    return model,tokenizer

if __name__ == '__main__':
    # model = AutoModelForSequenceClassification.from_pretrained("/mnt/public03/usr/sunyifei2/hugging_face/models--KoalaAI--Text-Moderation")
    model,tokenizer=load_model()
    # # # tokenizer = AutoTokenizer.from_pretrained('/mnt/public03/usr/sunyifei2/hugging_face/chinese-bert-wwm-ext')
    onnx_path='/mnt/nj-larc/usr/ajie1/sunyifei/nlp_codebase/logs/train/SunyifeiTanghuang48LConfig/20250321_153747/checkpoints/0/step_55000/onnx/model.onnx'
    onnx_fp16_path='/mnt/nj-larc/usr/ajie1/sunyifei/nlp_codebase/logs/train/SunyifeiTanghuang48LConfig/20250321_153747/checkpoints/0/step_55000/fp16/model_fp16.onnx'
    convert_onnx(model=model,tokenizer=tokenizer,onnx_path=onnx_path)
    convert_fp16(onnx_model_path=onnx_path,onnx_model_fp16_path=onnx_fp16_path)
    test_fp_16(onnx_model_fp16_path=onnx_fp16_path,tokenizer=tokenizer)
    # onnx_fp16_path="/mnt/public03/usr/sunyifei2/checkpoints/1205_youdao_video/step_22000/onnx_model/model_fp16.onnx"
    # predictor=Predictor(onnx_fp16_path=onnx_fp16_path)

    
    # test_file_path='/mnt/public03/usr/sunyifei2/engagement_bait/qs_deploy/check_datasets/1211_youdao_v1/test_for_online_1024_input_data_image_list.json'
    # with open(test_file_path, 'r',encoding='utf-8-sig') as f:
    #     datas = json.load(f)
    # save_path='/mnt/public03/usr/sunyifei2/engagement_bait/qs_deploy/check_datasets/1211_youdao_v1/ori_result_1212.csv'
    #predictor=Predictor(onnx_fp16_path=onnx_fp16_path)
    #predictor.predict(datas,save_path)
    # data= {
    #     "note_id": "67359a100000000019015804",
    #     "title": "ç»„äº†ä¸€ä¸ªå…¨å›½ç¦…ä¿®é«˜è´¨é‡åˆ›ä¸šäº¤æµqun",
    #     "content": "ğŸˆä¸€å®šè¦ç»™å¤§å®¶åˆ†äº«ä¸€ä¸ªè¶…æ£’çš„ç¦…ä¿®åˆ›ä¸šäº¤æµç¾¤ğŸ¥³ã€‚\t\nâœ¨åœ¨è¿™é‡Œï¼Œå‰åç«¯çš„ç¦…ä¿®åˆ›ä¸šè€…æ±‡èšä¸€å ‚ï¼Œæœ‰åˆ›å§‹äººğŸ‘¨â€ğŸ’¼ã€åˆä¼™äººğŸ‘©â€ğŸ¤â€ğŸ‘¨ã€æŠ•èµ„äººğŸ’¼ã€è¿è¥äººç­‰ç­‰ã€‚å¤§å®¶çœŸè¯šäº¤æµï¼Œæ¯«æ— ä¿ç•™åœ°åˆ†äº«åˆ›ä¸šç»éªŒğŸ’¡ã€å®æˆ˜å¹²è´§ğŸ“å’Œæœ€æ–°æ‰“æ³•ğŸ¯ã€‚\t\nğŸ’ªç¾¤é‡Œçš„æ°›å›´ç§¯æå‘ä¸Šï¼Œå¤§å®¶ä¸€èµ·é“¾æ¥èµ„æºğŸ—ï¸ï¼Œå…±äº«ç¦…ä¿®çš„è¡Œä¸šèµ„æºï¼Œå¯»æ‰¾åˆä½œæœºé‡ğŸ‰ã€‚åœ¨è¿™é‡Œï¼Œä½ ä¸å†æ˜¯ä¸€ä¸ªäººåœ¨æˆ˜æ–—ï¼Œè€Œæ˜¯æœ‰ä¸€ç¾¤å¿—åŒé“åˆçš„ä¼™ä¼´ä¸ä½ å¹¶è‚©å‰è¡ŒğŸš¶â€â™‚ï¸ğŸš¶â€â™€ï¸ã€‚\t\nğŸŒŸé™¤äº†çº¿ä¸Šçš„çƒ­çƒˆäº¤æµï¼Œæˆ‘ä»¬è¿˜ä¼šä¸å®šæœŸä¸¾åŠçº¿ä¸‹èšä¼šäº¤æµğŸ¥³ã€‚è®©ä½ æœ‰æœºä¼šé¢å¯¹é¢äº¤æµï¼Œæ‹“å±•äººè„‰èµ„æºğŸ‘¥ã€‚\t\nğŸ™Œæ¬¢è¿åŠ å…¥è¿™æ ·ä¸€ä¸ªç§¯æã€é•¿æœŸã€è®¤çœŸäº¤æµçš„ç¦…ä¿®åˆ›ä¸šåœˆå­\nÂ #ç¦…ä¿®[è¯é¢˜]#Â Â #å¹²è´§åˆ†äº«[è¯é¢˜]#Â Â #æ—…ä¿®[è¯é¢˜]#Â Â #åˆ›ä¸šä¸‰åå…­è®¡[è¯é¢˜]#Â Â #èµ„æºæ•´åˆ[è¯é¢˜]#Â \t\n",
    #     "taxonomy1": "èŒåœº",
    #     "taxonomy2": "èŒåœºè¡Œä¸š",
    #     "ocr": ", ç¦…ä¿®æ—…ä¿®, èµ„æºäº¤æ¢, è€åœ°, æ³•",
    #     "note_type": "1",
    #     "asr": "",
    #     "rn_b": 1,
    #     "note_id_d": "67359a100000000019015804",
    #     "ocr_image_text": "{\"http://ci.xiaohongshu.com/spectrum/1040g34o31a618t1jmm0g5pnoptlnc1ugm120le8?imageView2/2/w/1080/format/jpg\":\", ç¦…ä¿®æ—…ä¿®, èµ„æºäº¤æ¢, è€åœ°, æ³•\"}",
    #     "video_first_image_url": "http://ci.xiaohongshu.com/spectrum/1040g34o31a618t1jmm0g5pnoptlnc1ugm120le8?imageView2/2/w/1080/format/jpg",
    #     "image_url_list": "http://ci.xiaohongshu.com/spectrum/1040g34o31a618t1jmm0g5pnoptlnc1ugm120le8?imageView2/2/w/1080/format/jpg",
    #     "rn_d": 1
    # }
    # data=    {
    #     "note_id": "673598f300000000190198c8",
    #     "title": "ç´ é¾™è¿™ä¹ˆè´µ åˆ°åº•è°åœ¨ä¹°å•Š",
    #     "content": "æ ‡é¢˜å…šå“¦ æƒ³å…¥ä¸€ä¸ªç´ é¾™æŒ‚ä»¶ å°ç¿…è†€çš„æˆ´å¤´å¥—çš„åœ¨çŠ¹è±«ä¹°å“ªä¸ª\nçœ‹æ¼”å”±ä¼šå‰ï¼šä¸€ä¸ªæŒ‚ä»¶ æœ‰ç‚¹è´µ \nçœ‹æ¼”å”±ä¼šåï¼šç´ é¾™å„¿å­å¥½å¯çˆ±å•Š æ€§ä»·æ¯”è¶…é«˜å“’",
    #     "taxonomy1": "æ—¶å°š",
    #     "taxonomy2": "é…é¥°",
    #     "ocr": ", 14:28, <, æœæœ¬åº—å•†å“, æœç´¢, ç´ é¾™è¥ä¸šä¸­å‘¨è¾¹åº—Î¾, æ–°åº—å¼€ä¸š å·²å®Œæˆèµ„è´¨è®¤è¯, +å…³æ³¨, 2.5ä¸‡, 1188, 2ä¸ªä½œå“>, ç²‰ä¸, é”€é‡, æŠ–éŸ³å·, ç»¼åˆ, é”€é‡, æ–°å“, ä»·æ ¼, ä¸‡Â·ä¼ç‰¹, RowanTic, æ±ªè‹æ³·, RowanTic, Rowan TicåŒ…å­å¤´å¥—é¾™æ¯›ç»’æŒ‚ä»¶Î¾, åä¸‡è¾¾ç‰¹ä¸»é¢˜ç´ é¾™æŒ‚ä»¶ï¼ˆé—ªç”µç‰ˆï¼‰, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicåŒ…, *æ•ˆæœå›¾ä»…ä¾›å‚è€ƒï¼Œä»¥å®ç‰©æè´¨åŠå¤§å°ä¸ºå‡†, çƒ­æœåº¦è¶…99%åŒç±»å“, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicå, çƒ­æœåº¦è¶…99%åŒç±»å“, ï¿¥98å¼€å”®ä»·759äººæ”¶è—, ï¿¥119å¼€å”®ä»· 439äººæ”¶è—, 7å¤©æ— ç†ç”±é€€è´§ æé€Ÿé€€æ¬¾, 7å¤©æ— ç†ç”±é€€è´§ æé€Ÿé€€æ¬¾, ä¸‡Â·ä¼ç‰¹, RowanTicÎ¾, æ±ªè‹æ³·, RowanTicç´ é¾™æ¯›ç»’å…¬ä»”, åä¸‡ç‰¹å¿ƒç”µæ„Ÿåº”æ´æ£’ï¼ˆå·¦ï¼‰, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicç´ , ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicå, é¦–é¡µ, å…¨éƒ¨å•†å“, è”ç³»å®¢æœ",
    #     "note_type": "1",
    #     "asr": "",
    #     "rn_b": 1,
    #     "note_id_d": "673598f300000000190198c8",
    #     "ocr_image_text": "{\"http://ci.xiaohongshu.com/1040g2sg31a6158hl72705nrhnk308pq0l2l9rio?imageView2/2/w/1080/format/jpg\":\", 14:28, <, æœæœ¬åº—å•†å“, æœç´¢, ç´ é¾™è¥ä¸šä¸­å‘¨è¾¹åº—Î¾, æ–°åº—å¼€ä¸š å·²å®Œæˆèµ„è´¨è®¤è¯, +å…³æ³¨, 2.5ä¸‡, 1188, 2ä¸ªä½œå“>, ç²‰ä¸, é”€é‡, æŠ–éŸ³å·, ç»¼åˆ, é”€é‡, æ–°å“, ä»·æ ¼, ä¸‡Â·ä¼ç‰¹, RowanTic, æ±ªè‹æ³·, RowanTic, Rowan TicåŒ…å­å¤´å¥—é¾™æ¯›ç»’æŒ‚ä»¶Î¾, åä¸‡è¾¾ç‰¹ä¸»é¢˜ç´ é¾™æŒ‚ä»¶ï¼ˆé—ªç”µç‰ˆï¼‰, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicåŒ…, *æ•ˆæœå›¾ä»…ä¾›å‚è€ƒï¼Œä»¥å®ç‰©æè´¨åŠå¤§å°ä¸ºå‡†, çƒ­æœåº¦è¶…99%åŒç±»å“, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicå, çƒ­æœåº¦è¶…99%åŒç±»å“, ï¿¥98å¼€å”®ä»·759äººæ”¶è—, ï¿¥119å¼€å”®ä»· 439äººæ”¶è—, 7å¤©æ— ç†ç”±é€€è´§ æé€Ÿé€€æ¬¾, 7å¤©æ— ç†ç”±é€€è´§ æé€Ÿé€€æ¬¾, ä¸‡Â·ä¼ç‰¹, RowanTicÎ¾, æ±ªè‹æ³·, RowanTicç´ é¾™æ¯›ç»’å…¬ä»”, åä¸‡ç‰¹å¿ƒç”µæ„Ÿåº”æ´æ£’ï¼ˆå·¦ï¼‰, ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicç´ , ã€å®˜æ–¹æ­£ç‰ˆã€‘RowanTicå, é¦–é¡µ, å…¨éƒ¨å•†å“, è”ç³»å®¢æœ\"}",
    #     "video_first_image_url": "http://ci.xiaohongshu.com/1040g2sg31a6158hl72705nrhnk308pq0l2l9rio?imageView2/2/w/1080/format/jpg",
    #     "image_url_list": "http://ci.xiaohongshu.com/1040g2sg31a6158hl72705nrhnk308pq0l2l9rio?imageView2/2/w/1080/format/jpg",
    #     "rn_d": 1
    # }


    ##ori
    config=Config()
    # predictor=oriPredictor(config)
    #
    # onnx_fp16_path="/mnt/public03/usr/sunyifei2/checkpoints/1205_youdao_video/step_22000/onnx_model/model_fp16.onnx"
    predictor=Predictor(onnx_fp16_path)
    data=     {
        "note_id": "678c45410000000019004204",
        "title": "",
        "content": "ç¬¬ä¸€æ¬¡å‘å¸–ï¼Œæœ‰ç‚¹ç´§å¼ \n#fyp[è¯é¢˜]# #tiktok[è¯é¢˜]# #ban[è¯é¢˜]# #ä½ å¥½[è¯é¢˜]# #å—¡å—¡å£°[è¯é¢˜]# ",
        "note_type": "1",
        "ocr_image_text": "{\"http://ci.xiaohongshu.com/1040g00831cqlmark0c005psc80a393gc1j99mjg?imageView2/2/w/1080/format/jpg\":\", Ni hao huzz\"}",
        "video_first_image_url": "http://ci.xiaohongshu.com/1040g00831cqlmark0c005psc80a393gc1j99mjg?imageView2/2/w/1080/format/jpg",
        "image_url_list": "http://ci.xiaohongshu.com/1040g00831cqlmark0c005psc80a393gc1j99mjg?imageView2/2/w/1080/format/jpg",
        "taxonomy1": "ç”Ÿæ´»è®°å½•",
        "taxonomy2": "æ¥åœ°æ°”ç”Ÿæ´»",
        "asr": ""
    }
    predictor.predict_one_data(data)


    
    

